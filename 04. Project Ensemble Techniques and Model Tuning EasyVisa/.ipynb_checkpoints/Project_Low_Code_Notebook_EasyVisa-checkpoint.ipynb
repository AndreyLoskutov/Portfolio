{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "xJPSrx6D-Lft",
      "metadata": {
        "id": "xJPSrx6D-Lft"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=10>Data Science and Business Analytics</font></center>\n",
        "<center><font size=6>Ensemble Techniques and Model Tuning</font></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wv_XHSCN-Yl_",
      "metadata": {
        "id": "wv_XHSCN-Yl_"
      },
      "source": [
        "<center><img src=\"https://images.pexels.com/photos/7235894/pexels-photo-7235894.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\" width=\"800\" height=\"500\"></center>\n",
        "\n",
        "<center><font size=6>Visa Approval Facilitation</font></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**"
      ],
      "metadata": {
        "id": "yZvo8CHcetWN"
      },
      "id": "yZvo8CHcetWN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context"
      ],
      "metadata": {
        "id": "YRCgj9ghHzgH"
      },
      "id": "YRCgj9ghHzgH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\n",
        "\n",
        "The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\n",
        "\n",
        "OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment."
      ],
      "metadata": {
        "id": "oJfLniQ3H5QR"
      },
      "id": "oJfLniQ3H5QR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective"
      ],
      "metadata": {
        "id": "dedxylmbH7Pt"
      },
      "id": "dedxylmbH7Pt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\n",
        "\n",
        "The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data  scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:\n",
        "\n",
        "* Facilitate the process of visa approvals.\n",
        "* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status."
      ],
      "metadata": {
        "id": "ss_Ex7h0IA-Q"
      },
      "id": "ss_Ex7h0IA-Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description"
      ],
      "metadata": {
        "id": "RIpGEUIHICuI"
      },
      "id": "RIpGEUIHICuI"
    },
    {
      "cell_type": "markdown",
      "id": "empty-shanghai",
      "metadata": {
        "id": "empty-shanghai"
      },
      "source": [
        "The data contains the different attributes of employee and the employer. The detailed data dictionary is given below.\n",
        "\n",
        "* case_id: ID of each visa application\n",
        "* continent: Information of continent the employee\n",
        "* education_of_employee: Information of education of the employee\n",
        "* has_job_experience: Does the employee has any job experience? Y= Yes; N = No\n",
        "* requires_job_training: Does the employee require any job training? Y = Yes; N = No\n",
        "* no_of_employees: Number of employees in the employer's company\n",
        "* yr_of_estab: Year in which the employer's company was established\n",
        "* region_of_employment: Information of foreign worker's intended region of employment in the US.\n",
        "* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.\n",
        "* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\n",
        "* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position\n",
        "* case_status:  Flag indicating if the Visa was certified or denied"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note: This is a sample solution for the project. Projects will NOT be graded on the basis of how well the submission matches this sample solution. Projects will be graded on the basis of the rubric only."
      ],
      "metadata": {
        "id": "nfLWgp_EfRWy"
      },
      "id": "nfLWgp_EfRWy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Please read the instructions carefully before starting the project.**\n",
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_______' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same.\n"
      ],
      "metadata": {
        "id": "X6ZX7Rvfk1bJ"
      },
      "id": "X6ZX7Rvfk1bJ"
    },
    {
      "cell_type": "markdown",
      "id": "Lm7obbsV_RUT",
      "metadata": {
        "id": "Lm7obbsV_RUT"
      },
      "source": [
        "# **Importing necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the libraries with the specified version.\n",
        "!pip install numpy==1.25.2 pandas==1.5.3 scikit-learn==1.2.2 matplotlib==3.7.1 seaborn==0.13.1 xgboost==2.0.3 -q --user"
      ],
      "metadata": {
        "id": "6IOeGuQTMQXd"
      },
      "id": "6IOeGuQTMQXd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.*"
      ],
      "metadata": {
        "id": "OS2VAv465IZa"
      },
      "id": "OS2VAv465IZa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "canadian-maple",
      "metadata": {
        "id": "canadian-maple"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Libraries to help with reading and manipulating data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Library to split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# libaries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Removes the limit for the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Sets the limit for the number of displayed rows\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "\n",
        "# To oversample and undersample data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Libraries different ensemble classifiers\n",
        "from sklearn.ensemble import (\n",
        "    BaggingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    StackingClassifier,\n",
        ")\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Libraries to get different metric scores\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "# To tune different models\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thorough-passion",
      "metadata": {
        "id": "thorough-passion"
      },
      "source": [
        "# **Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment and run the following lines for Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kqF861_Ch2ea"
      },
      "id": "kqF861_Ch2ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alleged-teaching",
      "metadata": {
        "id": "alleged-teaching"
      },
      "outputs": [],
      "source": [
        "visa = pd.read_csv(\"_______\") ##  Fill the blank to read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "falling-annual",
      "metadata": {
        "id": "falling-annual"
      },
      "outputs": [],
      "source": [
        "# copying data to another variable to avoid any changes to original data\n",
        "data = visa.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mq-1s9p-_aKl",
      "metadata": {
        "id": "mq-1s9p-_aKl"
      },
      "source": [
        "# **Overview of the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aboriginal-wrist",
      "metadata": {
        "id": "aboriginal-wrist"
      },
      "source": [
        "## View the first and last 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "judicial-lease",
      "metadata": {
        "id": "judicial-lease"
      },
      "outputs": [],
      "source": [
        "data.'_______' ##  Complete the code to view top 5 rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "descending-david",
      "metadata": {
        "id": "descending-david"
      },
      "outputs": [],
      "source": [
        "data.'_______' ##  Complete the code to view last 5 rows of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessory-camel",
      "metadata": {
        "id": "accessory-camel"
      },
      "source": [
        "## Understand the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regulation-elder",
      "metadata": {
        "id": "regulation-elder"
      },
      "outputs": [],
      "source": [
        "data.'_______' ##  Complete the code to view dimensions of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assigned-berkeley",
      "metadata": {
        "id": "assigned-berkeley"
      },
      "source": [
        "## Check the data types of the columns for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "judicial-institute",
      "metadata": {
        "id": "judicial-institute"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for duplicate values"
      ],
      "metadata": {
        "id": "SbhR5Lt0Iv8v"
      },
      "id": "SbhR5Lt0Iv8v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "going-validation",
      "metadata": {
        "id": "going-validation"
      },
      "outputs": [],
      "source": [
        "# checking for duplicate values\n",
        "data.'_______' ##  Complete the code to check for duplicate values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for missing values"
      ],
      "metadata": {
        "id": "jqL9u2XPI0Z1"
      },
      "id": "jqL9u2XPI0Z1"
    },
    {
      "cell_type": "code",
      "source": [
        "data.'_______' ##  Complete the code to check for missing values"
      ],
      "metadata": {
        "id": "IgVLCHrsI3-U"
      },
      "id": "IgVLCHrsI3-U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "standing-horizontal",
      "metadata": {
        "id": "standing-horizontal"
      },
      "source": [
        "# <a name='link2'>**Exploratory Data Analysis (EDA)**</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "american-venue",
      "metadata": {
        "id": "american-venue"
      },
      "source": [
        "#### Let's check the statistical summary of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "premium-wagner",
      "metadata": {
        "id": "premium-wagner"
      },
      "outputs": [],
      "source": [
        "data.'_______' ##  Complete the code to print the statistical summary of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "competent-timing",
      "metadata": {
        "id": "competent-timing"
      },
      "source": [
        "#### Fixing the negative values in number of employees columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "whole-saying",
      "metadata": {
        "id": "whole-saying"
      },
      "outputs": [],
      "source": [
        "data.loc[data[\"no_of_employees\"] < 0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "waiting-shark",
      "metadata": {
        "id": "waiting-shark"
      },
      "source": [
        "* We will consider the 33 observations as data entry errors and take the absolute values for this column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prescription-bosnia",
      "metadata": {
        "id": "prescription-bosnia"
      },
      "outputs": [],
      "source": [
        "# taking the absolute values for number of employees\n",
        "data[\"no_of_employees\"] = abs(data[\"no_of_employees\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cutting-bookmark",
      "metadata": {
        "id": "cutting-bookmark"
      },
      "source": [
        "#### Let's check the count of each unique category in each of the categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "minute-helmet",
      "metadata": {
        "id": "minute-helmet"
      },
      "outputs": [],
      "source": [
        "# Making a list of all catrgorical variables\n",
        "cat_col = list(data.select_dtypes(\"object\").columns)\n",
        "\n",
        "# Printing number of count of each unique value in each column\n",
        "for column in cat_col:\n",
        "    print(data[column]._______) ##  Complete the code to print the count of each unique value in each column\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "occupied-classroom",
      "metadata": {
        "id": "occupied-classroom"
      },
      "outputs": [],
      "source": [
        "# checking the number of unique values\n",
        "data[\"case_id\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "existing-sociology",
      "metadata": {
        "id": "existing-sociology"
      },
      "outputs": [],
      "source": [
        "data.drop([\"case_id\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wooden-christian",
      "metadata": {
        "id": "wooden-christian"
      },
      "source": [
        "## Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions required for EDA"
      ],
      "metadata": {
        "id": "dQTwTg6IOGtT"
      },
      "id": "dQTwTg6IOGtT"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ],
      "metadata": {
        "id": "WlPoUrUxODTI"
      },
      "id": "WlPoUrUxODTI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "superb-springfield",
      "metadata": {
        "id": "superb-springfield"
      },
      "outputs": [],
      "source": [
        "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (15,10))\n",
        "    kde: whether to show the density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "editorial-command",
      "metadata": {
        "id": "editorial-command"
      },
      "source": [
        "### Observations on education of employee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "express-harrison",
      "metadata": {
        "id": "express-harrison"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"education_of_employee\", perc=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attempted-burlington"
      },
      "source": [
        "### Observations on region of employment"
      ],
      "id": "attempted-burlington"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suited-tuner"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(\"_______\") ## Complete the code to plot labeled barplot for the region of employment column"
      ],
      "id": "suited-tuner"
    },
    {
      "cell_type": "markdown",
      "id": "forbidden-kidney",
      "metadata": {
        "id": "forbidden-kidney"
      },
      "source": [
        "### Observations on job experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exterior-rings",
      "metadata": {
        "id": "exterior-rings"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(\"_______\") ## Complete the code to plot labeled barplot for the job experience column"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations on no_of_employees"
      ],
      "metadata": {
        "id": "HeKSMtIKpOmU"
      },
      "id": "HeKSMtIKpOmU"
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(data, 'no_of_employees', bins = 20)"
      ],
      "metadata": {
        "id": "eH5IgBQxpV1S"
      },
      "id": "eH5IgBQxpV1S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations on yr_of_estab"
      ],
      "metadata": {
        "id": "vkMV2OplqxT8"
      },
      "id": "vkMV2OplqxT8"
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(\"_______\")  ## Complete the code to plot histogram and boxplot for the year of establishment column"
      ],
      "metadata": {
        "id": "bW1wRUlUq0We"
      },
      "id": "bW1wRUlUq0We",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations on full_time_position"
      ],
      "metadata": {
        "id": "h6yD29-osL6x"
      },
      "id": "h6yD29-osL6x"
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(\"_______\") ## Complete the code to plot labeled barplot for the full time position column"
      ],
      "metadata": {
        "id": "X6r4dazUsUHg"
      },
      "id": "X6r4dazUsUHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stunning-surrey"
      },
      "source": [
        "### Observations on case status"
      ],
      "id": "stunning-surrey"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potential-stroke"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(\"_______\") ## Complete the code to plot labeled barplot for the case status column"
      ],
      "id": "potential-stroke"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "published-fourth"
      },
      "source": [
        "* 66.8% of the visas were certified."
      ],
      "id": "published-fourth"
    },
    {
      "cell_type": "markdown",
      "id": "equivalent-aging",
      "metadata": {
        "id": "equivalent-aging"
      },
      "source": [
        "## Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "social-wagner",
      "metadata": {
        "id": "social-wagner"
      },
      "outputs": [],
      "source": [
        "cols_list = data.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(\n",
        "    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blond-convertible",
      "metadata": {
        "id": "blond-convertible"
      },
      "source": [
        "**Creating functions that will help us with further analysis.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaptive-recipient",
      "metadata": {
        "id": "adaptive-recipient"
      },
      "outputs": [],
      "source": [
        "### function to plot distributions wrt target\n",
        "\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "third-sheriff",
      "metadata": {
        "id": "third-sheriff"
      },
      "outputs": [],
      "source": [
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dressed-excuse",
      "metadata": {
        "id": "dressed-excuse"
      },
      "source": [
        "#### Let's find out if education has any impact on visa certification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "special-florist",
      "metadata": {
        "id": "special-florist"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data, \"education_of_employee\", \"case_status\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attended-current"
      },
      "source": [
        "#### Lets' similarly check for the continents and find out how the visa status vary across different continents."
      ],
      "id": "attended-current"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alive-bloom"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(\"_______\") ## Complete the code to plot the stacked barplot for continent and case status"
      ],
      "id": "alive-bloom"
    },
    {
      "cell_type": "markdown",
      "id": "macro-decrease",
      "metadata": {
        "id": "macro-decrease"
      },
      "source": [
        "#### Let's see if having work experience has any influence over visa certification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "divine-dimension",
      "metadata": {
        "id": "divine-dimension"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(\"_______\") ## Complete the code to plot the stacked barplot for job experience and case status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "changing-kansas"
      },
      "source": [
        "#### Checking if the prevailing wage is similar across all the regions of the US"
      ],
      "id": "changing-kansas"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dangerous-pride"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(data=data, x=\"region_of_employment\", y=\"prevailing_wage\")\n",
        "plt.show()"
      ],
      "id": "dangerous-pride"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lesser-bacteria"
      },
      "source": [
        "#### Let's analyze the data and see if the visa status changes with the prevailing wage"
      ],
      "id": "lesser-bacteria"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perfect-haiti"
      },
      "outputs": [],
      "source": [
        "distribution_plot_wrt_target(data, \"prevailing_wage\", \"case_status\")"
      ],
      "id": "perfect-haiti"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations on no_of_employees and case_status"
      ],
      "metadata": {
        "id": "GASigPcj4XIe"
      },
      "id": "GASigPcj4XIe"
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(\"_______\")  ## Complete the code to plot the distribution plot wrt target for no of employees and case status"
      ],
      "metadata": {
        "id": "IcKLVIyl4ecI"
      },
      "id": "IcKLVIyl4ecI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations on yr_of_estab and case_status"
      ],
      "metadata": {
        "id": "awDMpLps5cKs"
      },
      "id": "awDMpLps5cKs"
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(\"_______\")  ## Complete the code to plot the distribution plot wrt target for year of establishment and case status"
      ],
      "metadata": {
        "id": "Cch4Vi-t5gvf"
      },
      "id": "Cch4Vi-t5gvf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suspected-asthma"
      },
      "source": [
        "#### Let's find out if it has any impact on visa applications getting certified."
      ],
      "id": "suspected-asthma"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "impossible-aquatic"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(\"_______\")  ## Complete the code to plot the stacked barplot for unit of wage and case status"
      ],
      "id": "impossible-aquatic"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations on full_time_position and case_status"
      ],
      "metadata": {
        "id": "0R3Bmnha8sK_"
      },
      "id": "0R3Bmnha8sK_"
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(\"_______\")  ## Complete the code to plot the stacked barplot for full time position and case status"
      ],
      "metadata": {
        "id": "Q2e2UGgM8wz4"
      },
      "id": "Q2e2UGgM8wz4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qBWlk20UBUAx",
      "metadata": {
        "id": "qBWlk20UBUAx"
      },
      "source": [
        "# **Data Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "allied-association",
      "metadata": {
        "id": "allied-association"
      },
      "source": [
        "### Outlier Check\n",
        "\n",
        "- Let's check for outliers in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "certified-complaint",
      "metadata": {
        "id": "certified-complaint"
      },
      "outputs": [],
      "source": [
        "# outlier detection using boxplot\n",
        "numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, variable in enumerate(numeric_columns):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.boxplot(data[variable], whis=1.5)\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flexible-independence",
      "metadata": {
        "id": "flexible-independence"
      },
      "source": [
        "### Data Preparation for modeling\n",
        "\n",
        "- We want to predict which visa will be certified.\n",
        "- Before we proceed to build a model, we'll have to encode categorical features.\n",
        "- We'll split the data into train and test to be able to evaluate the model that we build on the train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "genuine-evening",
      "metadata": {
        "id": "genuine-evening"
      },
      "outputs": [],
      "source": [
        "data[\"case_status\"] = data[\"case_status\"].apply(lambda x: 1 if x == \"Certified\" else 0)\n",
        "\n",
        "X = data.drop([\"case_status\"], axis=1)\n",
        "y = data[\"case_status\"]\n",
        "\n",
        "\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "# X = X.astype(float)\n",
        "\n",
        "# Splitting data into training, validation and test set:\n",
        "# first we split data into 2 parts, say temporary and test\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1, stratify=y\n",
        ")\n",
        "\n",
        "# then we split the temporary set into train and validation\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.25, random_state=1, stratify=y_temp\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "empty-typing",
      "metadata": {
        "id": "empty-typing"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of Training set : \", X_train.shape)\n",
        "print(\"Shape of the Validation set: \", X_val.shape)\n",
        "print(\"Shape of test set : \", X_test.shape)\n",
        "print(\"Percentage of classes in training set:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"Percentage of classes in validation set:\")\n",
        "print(y_val.value_counts(normalize=True))\n",
        "print(\"Percentage of classes in test set:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building**"
      ],
      "metadata": {
        "id": "-LgaIx5oJWY0"
      },
      "id": "-LgaIx5oJWY0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation criterion"
      ],
      "metadata": {
        "id": "dr7q6-dbfiQB"
      },
      "id": "dr7q6-dbfiQB"
    },
    {
      "cell_type": "markdown",
      "id": "foreign-yemen",
      "metadata": {
        "id": "foreign-yemen"
      },
      "source": [
        "**Model can make wrong predictions as**:\n",
        "\n",
        "1. Model predicts that the visa application will get certified but in reality, the visa application should get denied.\n",
        "2. Model predicts that the visa application will not get certified but in reality, the visa application should get certified.\n",
        "\n",
        "**Which case is more important?**\n",
        "* Both the cases are important as:\n",
        "\n",
        "* If a visa is certified when it had to be denied a wrong employee will get the job position while US citizens will miss the opportunity to work on that position.\n",
        "\n",
        "* If a visa is denied when it had to be certified the U.S. will lose a suitable human resource that can contribute to the economy.\n",
        "\n",
        "\n",
        "**How to reduce the losses?**\n",
        "\n",
        "* `F1 Score` can be used a the metric for evaluation of the model, greater the F1  score higher are the chances of minimizing False Negatives and False Positives.\n",
        "* We will use balanced class weights so that model focuses equally on both classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "capital-charlotte",
      "metadata": {
        "id": "capital-charlotte"
      },
      "source": [
        "First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
        "* The model_performance_classification_sklearn function will be used to check the model performance of models.\n",
        "* The confusion_matrix_sklearn function will be used to plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mexican-database",
      "metadata": {
        "id": "mexican-database"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "\n",
        "\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recreational-topic",
      "metadata": {
        "id": "recreational-topic"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    To plot the confusion_matrix with percentages\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(predictors)\n",
        "    cm = confusion_matrix(target, y_pred)\n",
        "    labels = np.asarray(\n",
        "        [\n",
        "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
        "            for item in cm.flatten()\n",
        "        ]\n",
        "    ).reshape(2, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building - Original Data"
      ],
      "metadata": {
        "id": "m4kesdX5Jtus"
      },
      "id": "m4kesdX5Jtus"
    },
    {
      "cell_type": "code",
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random Forest\", RandomForestClassifier(random_state=1, class_weight=\"balanced\")))\n",
        "'_______' ## Complete the code to append remaining 3 models in the list models\n",
        "\n",
        "\n",
        "print(\"\\n\" \"Training Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    scores = f1_score(y_train, model.predict(X_train))\n",
        "    print(\"{}: {}\".format(name, scores))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    scores_val = f1_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores_val))"
      ],
      "metadata": {
        "id": "KVy1OmumKYil"
      },
      "id": "KVy1OmumKYil",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building - Oversampled Data"
      ],
      "metadata": {
        "id": "EI6zAWgsLOLN"
      },
      "id": "EI6zAWgsLOLN"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before Oversampling, counts of label 'Certified': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before Oversampling, counts of label 'Denied': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "sm = SMOTE(\n",
        "    sampling_strategy=1, k_neighbors=5, random_state=1\n",
        ")  # Synthetic Minority Over Sampling Technique\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"After Oversampling, counts of label 'Certified': {}\".format(sum(y_train_over == 1)))\n",
        "print(\"After Oversampling, counts of label 'Denied': {} \\n\".format(sum(y_train_over == 0)))\n",
        "\n",
        "\n",
        "print(\"After Oversampling, the shape of train_X: {}\".format(X_train_over.shape))\n",
        "print(\"After Oversampling, the shape of train_y: {} \\n\".format(y_train_over.shape))"
      ],
      "metadata": {
        "id": "geQknLDRLSHE"
      },
      "id": "geQknLDRLSHE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random Forest\", RandomForestClassifier(random_state=1)))\n",
        "'_______' ## Complete the code to append remaining 3 models in the list models\n",
        "\n",
        "\n",
        "print(\"\\n\" \"Training Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over, y_train_over)\n",
        "    scores = f1_score(_____, model.predict(_____))  ## Complete the code to build models on oversampled data\n",
        "    print(\"{}: {}\".format(name, scores))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over, y_train_over)\n",
        "    scores = f1_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ],
      "metadata": {
        "id": "FVcfL-hDLxJB"
      },
      "id": "FVcfL-hDLxJB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building - Undersampled Data"
      ],
      "metadata": {
        "id": "IJOgkfyKMpAc"
      },
      "id": "IJOgkfyKMpAc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3a65f4c"
      },
      "outputs": [],
      "source": [
        "rus = RandomUnderSampler(random_state=1)\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"
      ],
      "id": "f3a65f4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "580b5abc"
      },
      "outputs": [],
      "source": [
        "print(\"Before Under Sampling, counts of label 'Certified': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before Under Sampling, counts of label 'Denied': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "print(\"After Under Sampling, counts of label 'Certified': {}\".format(sum(y_train_un == 1)))\n",
        "print(\"After Under Sampling, counts of label 'Denied': {} \\n\".format(sum(y_train_un == 0)))\n",
        "\n",
        "print(\"After Under Sampling, the shape of train_X: {}\".format(X_train_un.shape))\n",
        "print(\"After Under Sampling, the shape of train_y: {} \\n\".format(y_train_un.shape))"
      ],
      "id": "580b5abc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e674701b"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random Forest\", RandomForestClassifier(random_state=1)))\n",
        "'_______' ## Complete the code to append remaining 3 models in the list models\n",
        "\n",
        "\n",
        "print(\"\\n\" \"Training Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train_un, y_train_un)\n",
        "    scores = f1_score(____, model.predict(____))  ## Complete the code to build models on undersampled data\n",
        "    print(\"{}: {}\".format(name, scores))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train_un, y_train_un)\n",
        "    scores = f1_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ],
      "id": "e674701b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Performance Improvement**"
      ],
      "metadata": {
        "id": "p1hmd6aRPhCQ"
      },
      "id": "p1hmd6aRPhCQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Note**"
      ],
      "metadata": {
        "id": "pobuLTc5WGAb"
      },
      "id": "pobuLTc5WGAb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sample parameter grids have been provided to do necessary hyperparameter tuning. These sample grids are expected to provide a balance between model performance improvement and execution time. One can extend/reduce the parameter grid based on execution time and system configuration.\n",
        "  - Please note that if the parameter grid is extended to improve the model performance further, the execution time will increase\n",
        "2. The models chosen in this notebook are based on test runs. One can update the best models as obtained upon code execution and tune them for best performance.\n",
        "\n",
        "- For Gradient Boosting:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"init\": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"subsample\":[0.7,0.9],\n",
        "    \"max_features\":[0.5,0.7,1],\n",
        "}\n",
        "```\n",
        "\n",
        "- For Adaboost:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "```\n",
        "\n",
        "- For Bagging Classifier:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'max_samples': [0.8,0.9,1],\n",
        "    'max_features': [0.7,0.8,0.9],\n",
        "    'n_estimators' : [30,50,70],\n",
        "}\n",
        "```\n",
        "- For Random Forest:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50,110,25],\n",
        "    \"min_samples_leaf\": np.arange(1, 4),\n",
        "    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n",
        "    \"max_samples\": np.arange(0.4, 0.7, 0.1)\n",
        "}\n",
        "```\n",
        "\n",
        "- For Decision Trees:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'max_depth': np.arange(2,6),\n",
        "    'min_samples_leaf': [1, 4, 7],\n",
        "    'max_leaf_nodes' : [10, 15],\n",
        "    'min_impurity_decrease': [0.0001,0.001]\n",
        "}\n",
        "```\n",
        "\n",
        "- For XGBoost:\n",
        "\n",
        "```\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "pffTIX8rQc9X"
      },
      "id": "pffTIX8rQc9X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5L2Pf6sPzqq"
      },
      "source": [
        "## Hyperparameter Tuning - Random Forest"
      ],
      "id": "F5L2Pf6sPzqq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiusUPgyPzqr"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Choose the type of classifier.\n",
        "rf_tuned = RandomForestClassifier(random_state=1, oob_score=True, bootstrap=True)\n",
        "\n",
        "parameters = {\n",
        "    \"max_depth\": list(np.arange(5, 15, 5)),\n",
        "    \"max_features\": [\"sqrt\", \"log2\"],\n",
        "    \"min_samples_split\": [3, 5, 7],\n",
        "    \"n_estimators\": np.arange(10, 40, 10),\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(rf_tuned, parameters, scoring=acc_scorer, cv=5, n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_over, y_train_over)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "rf_tuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "rf_tuned.fit(X_train, y_train)"
      ],
      "id": "GiusUPgyPzqr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwjBuy2PPzqr"
      },
      "source": [
        "### Checking model performance on training set"
      ],
      "id": "vwjBuy2PPzqr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPXfCkYDPzqr"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(rf_tuned, X_train, y_train)"
      ],
      "id": "mPXfCkYDPzqr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1GRoZ4KPzqr"
      },
      "outputs": [],
      "source": [
        "rf_tuned_model_train_perf = model_performance_classification_sklearn(\n",
        "    rf_tuned, X_train, y_train\n",
        ")\n",
        "rf_tuned_model_train_perf"
      ],
      "id": "r1GRoZ4KPzqr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-wbKeRPzqr"
      },
      "source": [
        "### Checking model performance on validation set"
      ],
      "id": "Ng-wbKeRPzqr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPeqKfAoPzqr"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(rf_tuned, X_val, y_val)"
      ],
      "id": "RPeqKfAoPzqr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELuefvcRPzqr"
      },
      "outputs": [],
      "source": [
        "rf_tuned_model_val_perf = model_performance_classification_sklearn(\n",
        "    rf_tuned, X_val, y_val\n",
        ")\n",
        "rf_tuned_model_val_perf"
      ],
      "id": "ELuefvcRPzqr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh1p__IRPzqr"
      },
      "source": [
        "## Hyperparameter Tuning - AdaBoost Classifier"
      ],
      "id": "Uh1p__IRPzqr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygGCIIdnPzqs"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Choose the type of classifier.\n",
        "abc_tuned = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "    # Let's try different max_depth for base_estimator\n",
        "    \"estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": np.arange(0.01,0.1,0.05),\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter  combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(_______)  ## Complete the code to define the grid search object\n",
        "grid_obj = grid_obj.fit(X_train, y_train)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "abc_tuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "abc_tuned.fit(X_train, y_train)"
      ],
      "id": "ygGCIIdnPzqs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW0oZtFNPzqu"
      },
      "source": [
        "### Checking model performance on training set"
      ],
      "id": "MW0oZtFNPzqu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrLzSyH7Pzqu"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(_______)  ## Complete the code to plot the confusion matrix for the Adaboost Classifier on the training set"
      ],
      "id": "jrLzSyH7Pzqu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66vAwiFLPzqu"
      },
      "outputs": [],
      "source": [
        "abc_tuned_model_train_perf = model_performance_classification_sklearn(_______)  ## Complete the code to get the perfromance metrics for the Adaboost Classifier on the training set\n",
        "abc_tuned_model_train_perf"
      ],
      "id": "66vAwiFLPzqu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dmFv8m6Pzqu"
      },
      "source": [
        "### Checking model performance on validation set"
      ],
      "id": "4dmFv8m6Pzqu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NX-1lCjPzqv"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(_______)  ## Complete the code to plot the confusion matrix for the Adaboost Classifier on the validation set"
      ],
      "id": "6NX-1lCjPzqv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiMA5Fd1Pzqv"
      },
      "outputs": [],
      "source": [
        "abc_tuned_model_val_perf = model_performance_classification_sklearn(_______)  ## Complete the code to get the perfromance metrics for the Adaboost Classifier on the validation set\n",
        "abc_tuned_model_val_perf"
      ],
      "id": "FiMA5Fd1Pzqv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2mSp_QIPzqv"
      },
      "source": [
        "## Hyperparameter Tuning - Gradient Boosting Classifier"
      ],
      "id": "b2mSp_QIPzqv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVcLf79MPzqv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Choose the type of classifier.\n",
        "gbc_tuned = GradientBoostingClassifier(\n",
        "    init=AdaBoostClassifier(random_state=1), random_state=1\n",
        ")\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"subsample\": [0.7,0.9],\n",
        "    \"max_features\": [0.7, 0.8, 0.9, 1],\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(_______)  ## Complete the code to define the grid search object\n",
        "grid_obj = grid_obj.fit(X_train, y_train)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "gbc_tuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "gbc_tuned.fit(X_train, y_train)"
      ],
      "id": "rVcLf79MPzqv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jb0mcgzPzqv"
      },
      "source": [
        "### Checking model performance on training set"
      ],
      "id": "5Jb0mcgzPzqv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDz06NlsPzqv"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(_______)  ## Complete the code to plot the confusion matrix for the Gradient Boosting Classifier on the training set"
      ],
      "id": "PDz06NlsPzqv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79ovhJodPzqv"
      },
      "outputs": [],
      "source": [
        "gbc_tuned_model_train_perf = model_performance_classification_sklearn(_______)  ## Complete the code to get the perfromance metrics for the Gradient Boosting Classifier on the training set\n",
        "gbc_tuned_model_train_perf"
      ],
      "id": "79ovhJodPzqv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj2jW8XzPzqw"
      },
      "source": [
        "### Checking model performance on validation set"
      ],
      "id": "Mj2jW8XzPzqw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB_z5mQDPzqw"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(_______)  ## Complete the code to plot the confusion matrix for the Gradient Boosting Classifier on the validation set"
      ],
      "id": "HB_z5mQDPzqw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwC9_qvpPzqw"
      },
      "outputs": [],
      "source": [
        "gbc_tuned_model_val_perf = model_performance_classification_sklearn(_______)  ## Complete the code to get the perfromance metrics for the Gradient Boosting Classifier on the validation set\n",
        "gbc_tuned_model_val_perf"
      ],
      "id": "lwC9_qvpPzqw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM8M_5kuPzqw"
      },
      "source": [
        "## Hyperparameter Tuning - XGBoost Classifier"
      ],
      "id": "yM8M_5kuPzqw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go-Tn_8NPzqw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Choose the type of classifier.\n",
        "xgb_tuned = XGBClassifier(random_state=1, eval_metric=\"logloss\")\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"scale_pos_weight\": [1,2,5],\n",
        "    \"subsample\": [0.9, 1],\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"gamma\": [1,3]\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(_______)  ## Complete the code to define the grid search object\n",
        "grid_obj = grid_obj.fit(X_train_over, y_train_over)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "xgb_tuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "xgb_tuned.fit(X_train, y_train)"
      ],
      "id": "go-Tn_8NPzqw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPcek0HEPzqw"
      },
      "source": [
        "### Checking model performance on training set"
      ],
      "id": "OPcek0HEPzqw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Sh_sA-Pzqw"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(_______)  ## Complete the code to plot the confusion matrix for the XGBoost Classifier on the training set"
      ],
      "id": "Z9Sh_sA-Pzqw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjjsw1ANPzqw"
      },
      "outputs": [],
      "source": [
        "xgb_tuned_model_train_perf = model_performance_classification_sklearn(_______)  ## Complete the code to get the perfromance metrics for the XGBoost Classifier on the training set\n",
        "xgb_tuned_model_train_perf"
      ],
      "id": "Mjjsw1ANPzqw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY1tFkCkPzqx"
      },
      "source": [
        "### Checking model performance on validation set"
      ],
      "id": "YY1tFkCkPzqx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc53772WPzqx"
      },
      "outputs": [],
      "source": [
        "confusion_matrix_sklearn(_______)  ## Complete the code to plot the confusion matrix for the XGBoost Classifier on the validation set"
      ],
      "id": "Vc53772WPzqx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHuLuHPhPzqx"
      },
      "outputs": [],
      "source": [
        "xgb_tuned_model_val_perf = model_performance_classification_sklearn(_______)  ## Complete the code to get the perfromance metrics for the XGBoost Classifier on the validation set\n",
        "xgb_tuned_model_val_perf"
      ],
      "id": "hHuLuHPhPzqx"
    },
    {
      "cell_type": "markdown",
      "id": "mechanical-clarity",
      "metadata": {
        "id": "mechanical-clarity"
      },
      "source": [
        "# **Model Comparison and Final Model Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "royal-panel",
      "metadata": {
        "id": "royal-panel"
      },
      "source": [
        "## Comparing all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "future-irrigation",
      "metadata": {
        "id": "future-irrigation"
      },
      "outputs": [],
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        rf_tuned_model_train_perf.T,\n",
        "        abc_tuned_model_train_perf.T,\n",
        "        gbc_tuned_model_train_perf.T,\n",
        "        xgb_tuned_model_train_perf.T,\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Tuned Random Forest\",\n",
        "    \"Tuned Adaboost Classifier\",\n",
        "    \"Tuned Gradient Boost Classifier\",\n",
        "    \"XGBoost Classifier Tuned\",\n",
        "]\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collective-fireplace",
      "metadata": {
        "id": "collective-fireplace"
      },
      "outputs": [],
      "source": [
        "# validation performance comparison\n",
        "\n",
        "'_______' ## Write the code to compare the performance on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7326b66"
      },
      "outputs": [],
      "source": [
        "# Let's check the performance on test set\n",
        "'_______' ## Write the code to check the performance of best model on test data"
      ],
      "id": "e7326b66"
    },
    {
      "cell_type": "markdown",
      "id": "stone-venezuela",
      "metadata": {
        "id": "stone-venezuela"
      },
      "source": [
        "## Important features of the final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forty-mayor",
      "metadata": {
        "id": "forty-mayor"
      },
      "outputs": [],
      "source": [
        "feature_names = X_train.columns\n",
        "importances = _______  ## Complete the code to get the feature importances of the best model\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "congressional-knock",
      "metadata": {
        "id": "congressional-knock"
      },
      "source": [
        "# **Actionable Insights and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "biological-affiliation",
      "metadata": {
        "id": "biological-affiliation"
      },
      "source": [
        "\n",
        "\n",
        "*   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "5Aw-i9Z8gDgA"
      },
      "id": "5Aw-i9Z8gDgA"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yZvo8CHcetWN",
        "YRCgj9ghHzgH",
        "dedxylmbH7Pt",
        "RIpGEUIHICuI",
        "Lm7obbsV_RUT",
        "thorough-passion",
        "mq-1s9p-_aKl",
        "aboriginal-wrist",
        "accessory-camel",
        "assigned-berkeley",
        "SbhR5Lt0Iv8v",
        "jqL9u2XPI0Z1",
        "standing-horizontal",
        "american-venue",
        "competent-timing",
        "cutting-bookmark",
        "wooden-christian",
        "dQTwTg6IOGtT",
        "editorial-command",
        "attempted-burlington",
        "forbidden-kidney",
        "HeKSMtIKpOmU",
        "vkMV2OplqxT8",
        "h6yD29-osL6x",
        "stunning-surrey",
        "equivalent-aging",
        "dressed-excuse",
        "attended-current",
        "macro-decrease",
        "changing-kansas",
        "lesser-bacteria",
        "GASigPcj4XIe",
        "awDMpLps5cKs",
        "suspected-asthma",
        "0R3Bmnha8sK_",
        "qBWlk20UBUAx",
        "allied-association",
        "flexible-independence",
        "-LgaIx5oJWY0",
        "dr7q6-dbfiQB",
        "m4kesdX5Jtus",
        "EI6zAWgsLOLN",
        "IJOgkfyKMpAc",
        "p1hmd6aRPhCQ",
        "pobuLTc5WGAb",
        "F5L2Pf6sPzqq",
        "vwjBuy2PPzqr",
        "Ng-wbKeRPzqr",
        "Uh1p__IRPzqr",
        "MW0oZtFNPzqu",
        "4dmFv8m6Pzqu",
        "b2mSp_QIPzqv",
        "5Jb0mcgzPzqv",
        "Mj2jW8XzPzqw",
        "yM8M_5kuPzqw",
        "mechanical-clarity",
        "royal-panel",
        "stone-venezuela",
        "congressional-knock"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}